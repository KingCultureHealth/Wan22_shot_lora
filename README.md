# Wan22_shot_lora ğŸ¬

![Model Status](https://img.shields.io/badge/Status-Active-success) ![Task](https://img.shields.io/badge/Task-Text2Video%20%7C%20Image2Video-blue)

**Wan22_shot_lora** is a specialized Low-Rank Adaptation (LoRA) model designed for the Wan2.2 video generation architecture. 

Its primary goal is to **break the continuity** typically found in AI videos and introduce **dynamic shot changes, scene cuts, and transitions**. It works effectively for both Text-to-Video (T2V) and Image-to-Video (I2V) workflows.

## âœ¨ Key Features

*   **Cinematic Cuts:** Enables the model to generate videos that switch between different angles (e.g., Close-up â†’ Wide shot) or different scenes entirely.
*   **Dual Mode Support:**
    *   **Text-to-Video:** Describe a sequence of events, and the model will execute the cut.
    *   **Image-to-Video:** Start with an input image, and prompt the model to transition into a new scene.
*   **Enhanced Dynamics:** Reduces the "static" or "morphing" feel of standard video generation, creating a more edited, movie-like feel.

## ğŸ“¥ Download

*   **HuggingFace:** [Link to your HF repo]
*   **Civitai:** [Link to your Civitai page]

## ğŸ› ï¸ Usage

### Trigger Words
To activate the shot change effect, it is recommended to use the following trigger words in your prompt:
> **`é•œå¤´åˆ‡æ¢`**

### Recommended LoRA Weight
*   **Strength:** `0.6` to `1.0`
*   If the cut is too abrupt or glitchy, lower the weight. If the scene just morphs without a clear cut, increase the weight.

### Prompting Strategy (How to get the best results)

The key to getting a good shot change is to describe **two distinct states** in your prompt.

**Formula:**
`[Description of Scene A] + [é•œå¤´åˆ‡æ¢] + [Description of Scene B]`

**Examples:**
*   **T2V:** "ç‰¹å†™é•œå¤´å±•ç¤ºä¸€ä½å¥³æ€§çš„çœ¼ç›ï¼Œé•œå¤´åˆ‡æ¢åˆ°ä¸€å¹…å±•ç¤ºå¤œæ™šæœªæ¥èµ›åšæœ‹å…‹åŸå¸‚çš„å¹¿è§’æ— äººæœºé•œå¤´ã€‚"
*   **I2V (with input image of a car):** "æ±½è½¦é©¶ä¸‹é«˜é€Ÿå…¬è·¯ï¼Œé•œå¤´åˆ‡æ¢ï¼Œè½¬åœºåˆ°æµ·æ´‹ä¸Šç©ºçš„æ—¥è½æ™¯è±¡ã€‚"

  

https://github.com/user-attachments/assets/5dc36db9-5adf-4e51-a324-c042390d17a3

https://github.com/user-attachments/assets/7c2cc389-75d7-4e19-b879-0d8240859d66

### ComfyUI
1.  Load your standard Wan2.2 workflow.
2.  Insert a `Load LoRA` node.
3.  Connect **Wan22_shot_lora** to the main model path.
4.  Ensure your positive prompt includes the scene transition description.

### Diffusers (Python)
```python t2v
import torch
from PIL import Image
from diffsynth.utils.data import save_video, VideoData
from diffsynth.pipelines.wan_video import WanVideoPipeline, ModelConfig
from modelscope import dataset_snapshot_download


pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda:7",
    model_configs=[
        ModelConfig(model_id="Wan-AI/Wan2.2-T2V-A14B", origin_file_pattern="high_noise_model/diffusion_pytorch_model*.safetensors"),
        ModelConfig(model_id="Wan-AI/Wan2.2-T2V-A14B", origin_file_pattern="low_noise_model/diffusion_pytorch_model*.safetensors"),
        ModelConfig(model_id="Wan-AI/Wan2.2-T2V-A14B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth"),
        ModelConfig(model_id="Wan-AI/Wan2.2-T2V-A14B", origin_file_pattern="Wan2.1_VAE.pth"),
    ],
)
pipe.load_lora(pipe.dit, "step-1000.safetensors", alpha=1)

video = pipe(
    prompt="ç‰¹å†™é•œå¤´å±•ç¤ºä¸€ä½å¥³æ€§çš„çœ¼ç›ï¼Œé•œå¤´åˆ‡æ¢åˆ°ä¸€å¹…å±•ç¤ºå¤œæ™šæœªæ¥èµ›åšæœ‹å…‹åŸå¸‚çš„å¹¿è§’æ— äººæœºé•œå¤´ã€‚",
    negative_prompt="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°",
    num_frames=49,
    seed=1, tiled=True
)
save_video(video, "video_Wan2.2-T2V-A14B.mp4", fps=15, quality=5)
